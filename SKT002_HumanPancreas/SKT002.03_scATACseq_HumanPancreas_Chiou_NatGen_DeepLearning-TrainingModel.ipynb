{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"15\">\n",
    "SKT012.05.01.01.05_scATACseqChiouT2D_DeepLearning-TrainingModelStrideSize50Chromosomes_NewLibraryVersion-ExecutableScript\n",
    "</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mshap\u001b[49m\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(keras\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shap' is not defined"
     ]
    }
   ],
   "source": [
    "print(shap.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import optparse\n",
    "from array import *\n",
    "\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use('pdf')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import Bidirectional, Concatenate, PReLU \n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import regularizers\n",
    "#from keras.layers.recurrent import LSTM, Bidirectional, TimeDistributed\n",
    "from keras.layers import LSTM, Bidirectional, TimeDistributed\n",
    "from keras.layers import Layer, average, Input\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining essential functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(input_layer, hidden_layers):\n",
    "    output = input_layer\n",
    "    for hidden_layer in hidden_layers: \n",
    "        output = hidden_layer(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function get_output(input_layer, hidden_layers) takes in two arguments, input_layer and hidden_layers. The input_layer is the first layer of a neural network, and hidden_layers is a list of other layers in the network (i.e. the layers that come after the input layer and before the output layer). The function iterates through the hidden_layers and applies each layer to the input_layer one by one. The output of each hidden layer is passed as the input to the next hidden layer. The final output of the function is the output after passing the input through all the hidden layers. Essentially, this function is passing the input through a sequence of layers, and returning the final output after passing through all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your Model for Deep Learning\n",
    "def build_model():\n",
    "    forward_input = Input(shape=seq_shape)\n",
    "    reverse_input = Input(shape=seq_shape)\n",
    "\n",
    "    hidden_layers = [\n",
    "        Conv1D(1024, kernel_size=24, padding=\"valid\",\n",
    "               activation='relu', kernel_initializer='random_uniform'),\n",
    "        MaxPooling1D(pool_size=10, strides=10, padding='valid'), \n",
    "        Dropout(0.2),\n",
    "        TimeDistributed(Dense(1024, activation='relu')),\n",
    "        Bidirectional(LSTM(1024, dropout=0.1, recurrent_dropout=0.1, return_sequences=True)),\n",
    "        Dropout(0.2),\n",
    "        Flatten(),\n",
    "        Dense(2048, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(len(selected_classes), activation='sigmoid')]\n",
    "    \n",
    "    forward_output = get_output(forward_input, hidden_layers)     \n",
    "    reverse_output = get_output(reverse_input, hidden_layers)\n",
    "    output = average([forward_output, reverse_output])\n",
    "    model = Model(inputs=[forward_input, reverse_input], outputs=output)\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is defining a neural network model using the Keras library in Python. The model takes two inputs, \"forward_input\" and \"reverse_input\", which have the shape specified by the \"seq_shape\" variable. The model has several layers:\n",
    "\n",
    "* The first layer is a 1D convolutional layer with 128 filters, a kernel size of 20, and a \"valid\" padding. It uses the ReLU activation function and a random uniform initialization for the kernel weights.\n",
    "* The second layer is a max-pooling layer, with a pool size of 10 and stride of 10, using \"valid\" padding.\n",
    "* The third layer is a dropout layer with a dropout rate of 0.2, which is used to prevent overfitting.\n",
    "* The fourth layer is a time-distributed dense layer with 128 units and a ReLU activation function.\n",
    "* The fifth layer is a bidirectional LSTM layer with 128 units and dropout rate of 0.1 and recurrent dropout rate of 0.1, with return_sequence=True\n",
    "* The sixth layer is a dropout layer with a dropout rate of 0.2\n",
    "* The seventh layer is a flatten layer\n",
    "* The eighth layer is a dense layer with 256 units and a ReLU activation function\n",
    "* The nineth layer is a dropout layer with a dropout rate of 0.4\n",
    "* The final layer is a dense layer with the number of units equal to the length of the selected_classes variable, and a sigmoid activation function.\n",
    "\n",
    "The forward_output and reverse_output are obtained by passing their corresponding input through the hidden layers defined above. The final output of the model is obtained by taking the average of these two outputs. The model is then compiled with the Adam optimizer, binary cross-entropy loss function and accuracy metrics. The summary of the model is also printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filename):\n",
    "    ids = []\n",
    "    ids_d = {}\n",
    "    seqs = {}\n",
    "    classes = {}\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    seq = []\n",
    "    for line in lines:\n",
    "        if line[0] == '>':\n",
    "            ids.append(line[1:].rstrip('\\n'))\n",
    "            if line[1:].rstrip('\\n').split('_')[0] not in seqs:\n",
    "                seqs[line[1:].rstrip('\\n').split('_')[0]] = []\n",
    "            if line[1:].rstrip('\\n').split('_')[0] not in ids_d:\n",
    "                ids_d[line[1:].rstrip('\\n').split('_')[0]] = line[1:].rstrip('\\n').split('_')[0]\n",
    "            if line[1:].rstrip('\\n').split('_')[0] not in classes:\n",
    "                classes[line[1:].rstrip('\\n').split('_')[0]] = np.zeros(NUM_CLASSES)\n",
    "            classes[line[1:].rstrip('\\n').split('_')[0]][int(line[1:].rstrip('\\n').split('_')[1])-1] = 1        \n",
    "            if seq != []: seqs[ids[-2].split('_')[0]]= (\"\".join(seq))\n",
    "            seq = []\n",
    "        else:\n",
    "            seq.append(line.rstrip('\\n').upper())\n",
    "    if seq != []:\n",
    "        seqs[ids[-1].split('_')[0]]=(\"\".join(seq))\n",
    "    return ids,ids_d,seqs,classes\n",
    "\n",
    "def one_hot_encode_along_row_axis(sequence):\n",
    "    to_return = np.zeros((1,len(sequence),4), dtype=np.int8)\n",
    "    seq_to_one_hot_fill_in_array(zeros_array=to_return[0],\n",
    "                                 sequence=sequence, one_hot_axis=1)\n",
    "    return to_return\n",
    "\n",
    "def seq_to_one_hot_fill_in_array(zeros_array, sequence, one_hot_axis):\n",
    "    assert one_hot_axis==0 or one_hot_axis==1\n",
    "    if (one_hot_axis==0):\n",
    "        assert zeros_array.shape[1] == len(sequence)\n",
    "    elif (one_hot_axis==1): \n",
    "        assert zeros_array.shape[0] == len(sequence)\n",
    "    for (i,char) in enumerate(sequence):\n",
    "        if (char==\"A\" or char==\"a\"):\n",
    "            char_idx = 0\n",
    "        elif (char==\"C\" or char==\"c\"):\n",
    "            char_idx = 1\n",
    "        elif (char==\"G\" or char==\"g\"):\n",
    "            char_idx = 2\n",
    "        elif (char==\"T\" or char==\"t\"):\n",
    "            char_idx = 3\n",
    "        elif (char==\"N\" or char==\"n\"):\n",
    "            continue\n",
    "        else:\n",
    "            raise RuntimeError(\"Unsupported character: \"+str(char))\n",
    "        if (one_hot_axis==0):\n",
    "            zeros_array[char_idx,i] = 1\n",
    "        elif (one_hot_axis==1):\n",
    "            zeros_array[i,char_idx] = 1\n",
    "\n",
    "# Create plots for Accuracy or Loss for Training and Validation set for each epoch\n",
    "def create_plots(history):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.savefig(foldername + 'accuracy.png')\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.savefig(foldername + 'loss.png')\n",
    "    plt.clf()\n",
    "\n",
    "# Create Excel-file for Accuracy and Loss for Training and Validation set for each epoch\n",
    "def create_excel(history):\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df.index.name = 'Epoch'\n",
    "    excel_file = foldername + 'Accuracy_and_Loss.xlsx'\n",
    "    history_df.to_excel(excel_file)\n",
    "    \n",
    "def json_hdf5_to_model(json_filename, hdf5_filename):  \n",
    "    with open(json_filename, 'r') as f:\n",
    "        model = model_from_json(f.read())\n",
    "    model.load_weights(hdf5_filename)\n",
    "    return model\n",
    "\n",
    "def loc_to_model_loss(loc):\n",
    "    return json_hdf5_to_model(loc + 'model.json', loc + 'model_best_loss.hdf5')\n",
    "\n",
    "\n",
    "# randomly shuffle the order of labels in a dataset for cross-validation, training or testing\n",
    "def shuffle_label(label):\n",
    "    for i in range(len(label.T)):\n",
    "        label.T[i] = shuffle(label.T[i])\n",
    "    return label\n",
    "\n",
    "def calculate_roc_pr(score, label):\n",
    "    output = np.zeros((len(label.T), 2))\n",
    "    for i in range(len(label.T)):\n",
    "        roc_ = roc_auc_score(label.T[i], score.T[i])\n",
    "        pr_ = average_precision_score(label.T[i], score.T[i])\n",
    "        output[i] = [roc_, pr_]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare input...\n"
     ]
    }
   ],
   "source": [
    "# Define your input data\n",
    "NUM_CLASSES = 97\n",
    "selected_classes = np.arange(NUM_CLASSES)\n",
    "SEQ_LEN = 500\n",
    "SEQ_DIM = 4\n",
    "seq_shape = (SEQ_LEN, SEQ_DIM)\n",
    "\n",
    "# Define working folders\n",
    "InputFolder = '/lustre1/project/stg_00041/SanKit/SKT012_Chiou_snATAC_Pancreas/Data/DeepLearning/InputFileHumanPancreas/'\n",
    "foldername = '/lustre1/project/stg_00041/SanKit/SKT012_Chiou_snATAC_Pancreas/Data/DeepLearning/OutputFileHumanPancreas/TrainedModelStrideSize50Chromosomes/NewVersion/'\n",
    "\n",
    "\n",
    "# Define your Training and Validation dataset\n",
    "train_filename = InputFolder + 'MergedFile_WoChr2_WoChr11_extendedStrideSize50.fa'\n",
    "valid_filename = InputFolder + 'MergedFile_Chr2.fa'\n",
    "\n",
    "\n",
    "# Define the path for saving models\n",
    "PATH_TO_SAVE_ARC = foldername + 'model.json'\n",
    "PATH_TO_SAVE_BEST_LOST_WEIGHTS = foldername + 'model_best_loss.hdf5'\n",
    "PATH_TO_SAVE_BEST_ACC_WEIGHTS = foldername + 'model_best_acc.hdf5'\n",
    "PATH_TO_SAVE_END_WEIGHTS = foldername + 'model_end.hdf5'\n",
    "\n",
    "# Prepare your Training and Validation input\n",
    "print(\"Prepare input...\")\n",
    "train_ids, train_ids_d, train_seqs, train_classes = readfile(train_filename)\n",
    "X_train = np.array([one_hot_encode_along_row_axis(train_seqs[id]) for id in train_ids_d]).squeeze(axis=1)\n",
    "y_train = np.array([train_classes[id] for id in train_ids_d])\n",
    "y_train = y_train[:,selected_classes]\n",
    "X_train = X_train[y_train.sum(axis=1)>0]\n",
    "y_train = y_train[y_train.sum(axis=1)>0]\n",
    "X_train_rc = X_train[:,::-1,::-1]\n",
    "train_data = [X_train, X_train_rc]\n",
    "\n",
    "valid_ids, valid_ids_d, valid_seqs, valid_classes = readfile(valid_filename)\n",
    "X_valid = np.array([one_hot_encode_along_row_axis(valid_seqs[id]) for id in valid_ids_d]).squeeze(axis=1)\n",
    "y_valid = np.array([valid_classes[id] for id in valid_ids_d])\n",
    "y_valid = y_valid[:,selected_classes]\n",
    "X_valid = X_valid[y_valid.sum(axis=1)>0]\n",
    "valid_data = y_valid[y_valid.sum(axis=1)>0]\n",
    "X_valid_rc = X_valid[:,::-1,::-1]\n",
    "valid_data = [X_valid, X_valid_rc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Compile model...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 500, 4)]     0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 500, 4)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 477, 1024)    99328       ['input_7[0][0]',                \n",
      "                                                                  'input_8[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 47, 1024)    0           ['conv1d_3[0][0]',               \n",
      "                                                                  'conv1d_3[1][0]']               \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 47, 1024)     0           ['max_pooling1d_3[0][0]',        \n",
      "                                                                  'max_pooling1d_3[1][0]']        \n",
      "                                                                                                  \n",
      " time_distributed_3 (TimeDistri  (None, 47, 1024)    1049600     ['dropout_9[0][0]',              \n",
      " buted)                                                           'dropout_9[1][0]']              \n",
      "                                                                                                  \n",
      " bidirectional_3 (Bidirectional  (None, 47, 2048)    16785408    ['time_distributed_3[0][0]',     \n",
      " )                                                                'time_distributed_3[1][0]']     \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 47, 2048)     0           ['bidirectional_3[0][0]',        \n",
      "                                                                  'bidirectional_3[1][0]']        \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 96256)        0           ['dropout_10[0][0]',             \n",
      "                                                                  'dropout_10[1][0]']             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 2048)         197134336   ['flatten_3[0][0]',              \n",
      "                                                                  'flatten_3[1][0]']              \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 2048)         0           ['dense_10[0][0]',               \n",
      "                                                                  'dense_10[1][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 97)           198753      ['dropout_11[0][0]',             \n",
      "                                                                  'dropout_11[1][0]']             \n",
      "                                                                                                  \n",
      " average_3 (Average)            (None, 97)           0           ['dense_11[0][0]',               \n",
      "                                                                  'dense_11[1][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 215,267,425\n",
      "Trainable params: 215,267,425\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "-------------------------------------------\n",
      "Model architecture saved to.. /lustre1/project/stg_00041/SanKit/SKT012_Chiou_snATAC_Pancreas/Data/DeepLearning/OutputFileHumanPancreas/TrainedModelStrideSize50Chromosomesmodel.json\n",
      "-------------------------------------------\n",
      "Train model...\n",
      "Epoch 1/50\n",
      "  31/3556 [..............................] - ETA: 28:51:45 - loss: 0.4292 - accuracy: 0.0123"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-------------------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalid_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Plot Accuracy and Loss information\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-------------------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/data/leuven/332/vsc33293/miniconda/envs/Python_TFMODISCO/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/data/leuven/332/vsc33293/miniconda/envs/Python_TFMODISCO/lib/python3.9/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/data/leuven/332/vsc33293/miniconda/envs/Python_TFMODISCO/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/data/leuven/332/vsc33293/miniconda/envs/Python_TFMODISCO/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/data/leuven/332/vsc33293/miniconda/envs/Python_TFMODISCO/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/data/leuven/332/vsc33293/miniconda/envs/Python_TFMODISCO/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leuven/332/vsc33293/miniconda/envs/Python_TFMODISCO/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/data/leuven/332/vsc33293/miniconda/envs/Python_TFMODISCO/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/data/leuven/332/vsc33293/miniconda/envs/Python_TFMODISCO/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define your model\n",
    "print('-------------------------------------------')\n",
    "print(\"Compile model...\")\n",
    "model = build_model()\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(PATH_TO_SAVE_ARC, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "\n",
    "\n",
    "# Save the weights of the model after each EPOCH \n",
    "# if it has the best validation loss or validation accuracy    \n",
    "print('-------------------------------------------')\n",
    "print(\"Model architecture saved to..\", PATH_TO_SAVE_ARC)\n",
    "checkpoint1 = ModelCheckpoint(PATH_TO_SAVE_BEST_LOST_WEIGHTS,\n",
    "                              monitor='val_loss', verbose=1,\n",
    "                              save_best_only=True, mode='min')\n",
    "checkpoint2 = ModelCheckpoint(PATH_TO_SAVE_BEST_ACC_WEIGHTS,\n",
    "                              monitor='val_accuracy', verbose=1,\n",
    "                              save_best_only=True, mode='max')\n",
    "checkpoint3 = EarlyStopping(monitor='val_loss', patience=6)\n",
    "callbacks_list = [checkpoint1, checkpoint2, checkpoint3]\n",
    "\n",
    "\n",
    "# Define parameters\n",
    "EPOCH = 50\n",
    "BATCH = 128\n",
    "\n",
    "\n",
    "# Train Model\n",
    "print('-------------------------------------------')\n",
    "print(\"Train model...\")\n",
    "history = model.fit( train_data, y_train, epochs=EPOCH,\n",
    "                    batch_size=BATCH, shuffle=True,\n",
    "                    validation_data=(valid_data, y_valid),\n",
    "                    verbose=1, callbacks= callbacks_list)\n",
    "\n",
    "\n",
    "# Plot Accuracy and Loss information\n",
    "print('-------------------------------------------')\n",
    "print(\"Save Plots...\")\n",
    "create_plots(history)\n",
    "# Save Accuracy and Loss as Excel-file\n",
    "print('-------------------------------------------')\n",
    "print(\"Save Excel...\")\n",
    "create_excel(history)\n",
    "\n",
    "\n",
    "\n",
    "model.save_weights(PATH_TO_SAVE_END_WEIGHTS)\n",
    "print(\"Model weights saved to..\", PATH_TO_SAVE_END_WEIGHTS)\n",
    "plot_model(model, to_file=foldername + 'model.png')\n",
    "\n",
    "score, acc = model.evaluate(valid_data, y_valid, batch_size=BATCH)\n",
    "print('Test score:', score)\n",
    "print('Validation accuracy:', acc)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python_DeepPancreas_gpu_TF1.15]",
   "language": "python",
   "name": "conda-env-Python_DeepPancreas_gpu_TF1.15-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
